---
title: "Principal Componnet Analysis"
format: revealjs
 # pdf: default

---

This is to test quarto document
```{python}
import numpy as np

```
Machine Learning algorithm
```{python}
import pandas as pd
```

# Introduction {background-color="#008080"}

## What is Demensional Reduction?

Principal Component Analysis (PCA) is an ancient dimension reduction technique from statistics that may be motivated from different perspectives:

- maximizing projected variance
- minimizing reconstruction error

## The big Pictuer 

The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set which consists of a large number of interrelated variables, while retaining as much as possible of the variation present in the data set.

This is achieved by transforming to a new set of variables, the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.

Jolliffe 1986

## Under the hood
:::{.incremental}
PCA can be performed using

- spectral decomposition of a data covariance (or correlation) matrix or

- singular value decomposition of a data matrix,

possibly after a normalization step of the initial data.

The main device is a matrix factorization method known as Singular Value Decomposition (SVD)
:::
# Singular Value Decomposition

## 

The next theorem tells us that any real-valued matrix can be factorized into the product of two orthogonal matrices and a diagonal matrix

This factorization is at the core of many results and methods in computational matrix analysis.

From a computational viewpoint, Principal Component Analysis (PCA) is another name for Singular Value Decomposition (SVD)

## <span style="color:blue;">Singular Value Decomposition Theorem</span># 

Let $X$ be a $n \times p$ real matrix
There exists $U, D, V$

::: {.callout-tip}
$$
X=U \times D \times V^T \quad \text { with } \begin{cases}U \times U^T & =\operatorname{Id}_n \\ V \times V^T & =\operatorname{Id}_p \\ D & \text { non-negative diagonal. }\end{cases}
$$
:::

- The columns of $U$ are called left singular vectors.
- The columns of $V$ are called the right singular vectors.
- The diagonal coefficients of $D$ are called the singular values.
- testing

# <span style="color:blue;">ML Algorithm</span># 

::: {.incremental}
::: {.callout-tip}
## Key quantities in classification
$$\mathbb{P}(Y=1|X=\text{x})\propto \color{red}{\mathbb{P}(X=\text{x}|Y=1)}\times\color{green}{\mathbb{P}(Y=1)}.$$

- $\color{green}{\mathbb{P}(Y=1)}$ can be estimated by $\frac{n(\text{spams})}{n(\text{emails})}$ ✅

- $\color{red}{\mathbb{P}(X=\text{x}|Y=1)}$ is more complicated (**key to different models**) ⚠️
:::
:::

## <span style="color:blue;">Eigenvalue</span># 
:::: {.columns}

::: {.column width="40%"}
- This is for column 1
- Mathematical computation
:::

::: {.column width="60%"}
- This is for column 2
- Some image 
:::

::::

## Test 

::: {.incremental}

- Eat spaghetti
- Drink wine

:::